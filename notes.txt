== Current Research ==
https://people.ece.ubc.ca/sasha/papers/eurosys16-final29.pdf
https://ieeexplore.ieee.org/abstract/document/9651303
https://openaccess.inaf.it/handle/20.500.12386/33616
https://arxiv.org/abs/2401.14550
https://arxiv.org/abs/2204.13543
https://blog.chromium.org/2008/09/multi-process-architecture.html
https://ieeexplore.ieee.org/document/9651303
https://www.cybertec-postgresql.com/en/pg_timetable-advanced-postgresql-job-scheduling/
https://www.computer.org/csdl/proceedings-article/hipcw/2024/091100a068/24MFoMM1vRS : Predictive Modeling of Performance Variability in HPC Applications

== Simulators ==
-- C. Galleguillos, Z. Kiziltan, A. Netti, and R. Soto, "Accasim: a customizable workload management simulator for job dispatching research in HPC systems," Cluster Computing, vol. 23, no. 1, pp. 107–122, 2020.
https://link.springer.com/article/10.1007/s10586-019-02905-5
-- P.-F. Dutot, M. Mercier, M. Poquet, and O. Richard, "Batsim: a realistic language-independent resources and jobs management systems simulator," in Proc. of JSSPP Workshop 2015. Springer, 2015, pp. 178–197.
https://link.springer.com/chapter/10.1007/978-3-319-61756-5_10
-- D. Klusacek, M. Soysal, and F. Suter, "Alea–complex job scheduling simulator," in Proc. of PPAM 2019. Springer, 2019, pp. 217–229

== Simulator Resources ==
https://stackoverflow.com/questions/27685320/what-is-the-minimum-number-of-computers-for-a-slurm-cluster
https://hpckp.org/articles/how-to-use-the-slurm-simulator-as-a-development-and-testing-environment/
https://github.com/ubccr-slurm-simulator/slurm_sim_tools

== Other Resources ==
https://egyankosh.ac.in/bitstream/123456789/20792/1/Unit-6.pdf
https://web2.uwindsor.ca/math/hlynka/qonline.html
https://thenewstack.io/an-introduction-to-queue-theory-why-disaster-happens-at-the-edges/

== Ideas and Plans ==
- Hypothetical scenarios of 10,20,30 % of jobs that need urgent resources.
- Draw a spectrum of types of jobs that need resources urgently on one extreme to "don't care" on other extreme.

== Major Contributions of the study ==
- In depth analysis of the behavior of the various job-scheduling schemes in the presence of urgent computing
- A simple framework for performing simulations of job submissions by dummy users in a safe / guarded / controlled setup and customizable tooling knobs.
- A number of optimized and fine tuned slurm configs that could be readily adapted to suit needs for a given set of urgent computing requirements.
- A detailed literature review of the field.

== Useful Slurm query tools ==
sinfo
sacctmgr show qos

== Past Work ==
/Users/km0/slurm_queue_testing
/Users/km0/gitrepos/stochsimsched

==
Notes from Meeting with Thomas and Terry
- Co-scheduling for instruments when reservations are in place.
- Resulting in a dynamic QoS for Slurm
- A wide variety of jobs that may be classified as communication / IO / Compute heavy applications
- A negotiation protocol may be implemented for preemption -- wide variety of strategies are possible
- Relates to work by Tanzima Islam at Texas State University
  - Application patterns such as power profile
- Can some kind of telemetry, such as Darshan identify the application phases such as IO, Communication, Computation
- Can we classify applications into checkpointable and non-checkpointable? Typically, single-node, serial applications are easier to checkpoint
- How much of this can be integrated / automated within Slurm?
- S3M uses Slurm's REST api exclusively as far as scheduling goes
- The concept / idea of "Utility Curves" for applications
- Incorporating reservations and QoS in tandem so users don't bombard higher QoS -- many scenarios possible
- Quantum resources and scheduling may form an interesting challenge -- best interleaving strategy
- Three interleavable resources: Parallel File System | Power | Interconnect
- How to integrate AI to ease / enhance / automate some of these efforts?


=== Workflow to analyze existing Slurm data ===
Workflows are end-to-end solutions to complex problems. They are a reproducible unit of communication of the work that is done. 
Scheduling itself is a complex problem, especially the HPC job scheduling that is shaped by various contesting forces. These include the user expectations, the scheduling policies, the desire to optimially utilize the resources the app requirements and admin policies, maybe. The nature of the jobs that comes under application requirements so that becomes a complex problem. There is no systematic analysis that has been done on existing jobs in a given hpc center. 

in this paper we provide a workflow that implements a readymade recipe for extensive analysis of current jobs that will give readily insights on the jobs that have already run. We hope that this workflow will be useful for the community to analyze a site's historical jobs and it will provide insights on how to the overall performance of a center and inform shaping of the future policies of a given center.

- Produce a comprehensive chart where each job is represented as a multi-layer pie chart that is then plotted into two dimensions of values / properties. Example of a multi-level pie chart here: https://youtu.be/jaAGX3MiU6c ; Another example: https://www2.microstrategy.com/producthelp/current/MSTRWeb/webhelp/lang_1033/Content/Creating_a_graph_with_pies_or_rings.htm
https://www.reddit.com/r/dataisbeautiful/comments/1kc6mal/australian_houses_are_huge_oc/

- Produce a user oriented evaluation. How did top 1000 users' jobs fared over a period of time, say 1 year. wait times, run times, end status of jobs.

Workflow stages:

IDEA: Maybe call it AI assisted workflows. Along the philosophy that AI is supposed to assist the humans.

Stage 1. Obtain data (params: fetch / read from disc; columnset:, Date-Range, append to disk data)
      # the append option is interesting because then the entire dataset won't be needed to be fetched repeatitively.
      # need to find how many jobs on average do frontier receive every day.
Stage 2. Clean data (convert K / M / G etc to raw numbers, remove missing data where applicable)
Stage 3. Plot data
Stage 4. Feed the plots to an AI for analysis
Stage 5. Piecewise analysis based on timeperiods: daily / weekly / monthly / yearly

== Cite text from this abstract from F. Suter's paper: Improving Fairness in a Large Scale HTC system Through Workload Analysis and Simulation
Monitoring and analyzing the execution of a workload is at
the core of the operation of data centers. It allows operators to verify
that the operational objectives are satisfied or detect and react to any
unexpected and unwanted behavior. However, the scale and complexity
of large workloads composed of millions of jobs executed each month
on several thousands of cores, often limit the depth of such an analysis.
This may lead to overlook some phenomena that, while not harmful at
a global scale, can be detrimental to a specific class of users.
In this paper, we illustrate such a situation by analyzing a large High
Throughput Computing (HTC) workload trace coming from one of the
largest academic computing centers in France. The Fair-Share algorithm
at the core of the batch scheduler ensures that all user groups are fairly
provided with an amount of computing resources commensurate to their
expressed needs. However, a deeper analysis of the produced schedule,
especially of the job waiting times, shows a certain degree of unfairness
between user groups. We identify the configuration of the quotas and
scheduling queues as the main root causes of this unfairness. We thus
propose a drastic reconfiguration of the system that aims at being more
suited to the characteristics of the workload and at better balancing
the waiting time among user groups. We evaluate the impact of this
reconfiguration through detailed simulations. The obtained results show
that it still satisfies the main operational objectives while significantly
improving the quality of service experienced by formerly unfavored users.

== Time to pull data from the slurm database ==
- 849 seconds for 2022 data
- 964 seconds for 2023 data
- 603 seconds for 2024 data

 
