== Current Research ==
https://people.ece.ubc.ca/sasha/papers/eurosys16-final29.pdf
https://ieeexplore.ieee.org/abstract/document/9651303
https://openaccess.inaf.it/handle/20.500.12386/33616
https://arxiv.org/abs/2401.14550
https://arxiv.org/abs/2204.13543
https://blog.chromium.org/2008/09/multi-process-architecture.html
https://ieeexplore.ieee.org/document/9651303
https://www.cybertec-postgresql.com/en/pg_timetable-advanced-postgresql-job-scheduling/
https://www.computer.org/csdl/proceedings-article/hipcw/2024/091100a068/24MFoMM1vRS : Predictive Modeling of Performance Variability in HPC Applications

== Other Resources ==
https://egyankosh.ac.in/bitstream/123456789/20792/1/Unit-6.pdf
https://web2.uwindsor.ca/math/hlynka/qonline.html
https://thenewstack.io/an-introduction-to-queue-theory-why-disaster-happens-at-the-edges/

== Ideas and Plans ==
- Hypothetical scenarios of 10,20,30 % of jobs that need urgent resources.
- Draw a spectrum of types of jobs that need resources urgently on one extreme to "don't care" on other extreme.


== Major Contributions of the study ==
- In depth analysis of the behavior of the various job-scheduling schemes in the presence of urgent computing
- A simple framework for performing simulations of job submissions by dummy users in a safe / guarded / controlled setup and customizable tooling knobs.
- A number of optimized and fine tuned slurm configs that could be readily adapted to suit needs for a given set of urgent computing requirements.
- A detailed literature review of the field.

== Useful Slurm query tools ==
sinfo
sacctmgr show qos

== Past Work ==
/Users/km0/slurm_queue_testing
/Users/km0/gitrepos/stochsimsched

==
Notes from Meeting with Thomas and Terry
- Co-scheduling for instruments when reservations are in place.
- Resulting in a dynamic QoS for Slurm
- A wide variety of jobs that may be classified as communication / IO / Compute heavy applications
- A negotiation protocol may be implemented for preemption -- wide variety of strategies are possible
- Relates to work by Tanzima Islam at Texas State University
  - Application patterns such as power profile
- Can some kind of telemetry, such as Darshan identify the application phases such as IO, Communication, Computation
- Can we classify applications into checkpointable and non-checkpointable? Typically, single-node, serial applications are easier to checkpoint
- How much of this can be integrated / automated within Slurm?
- S3M uses Slurm's REST api exclusively as far as scheduling goes
- The concept / idea of "Utility Curves" for applications
- Incorporating reservations and QoS in tandem so users don't bombard higher QoS -- many scenarios possible
- Quantum resources and scheduling may form an interesting challenge -- best interleaving strategy
- Three interleavable resources: Parallel File System | Power | Interconnect
- How to integrate AI to ease / enhance / automate some of these efforts?


=== Workflow to analyze existing Slurm data ===
Workflows are end-to-end solutions to complex problems. They are a reproducible unit of communication of the work that is done. 
Scheduling itself is a complex problem, especially the HPC job scheduling that is shaped by various contesting forces. These include the user expectations, the scheduling policies, the desire to optimially utilize the resources the app requirements and admin policies, maybe. The nature of the jobs that comes under application requirements so that becomes a complex problem. There is no systematic analysis that has been done on existing jobs in a given hpc center. 

in this paper we provide a workflow that implements a readymade recipe for extensive analysis of current jobs that will give readily insights on the jobs that have already run. We hope that this workflow will be useful for the community to analyze a site's historical jobs and it will provide insights on how to the overall performance of a center and inform shaping of the future policies of a given center.

- Produce a user oriented evaluation. How did top 1000 users' jobs fared over a period of time, say 1 year. wait times, run times, end status of jobs.

Workflow stages:

IDEA: Maybe call it AI assisted workflows. Along the philosophy that AI is supposed to assist the humans.

Stage 1. Obtain data (params: fetch / read from disc; columnset:, Date-Range, append to disk data)
      # the append option is interesting because then the entire dataset won't be needed to be fetched repeatitively.
      # need to find how many jobs on average do frontier receive every day.
Stage 2. Clean data (convert K / M / G etc to raw numbers, remove missing data where applicable)
Stage 3. Plot data
Stage 4. Feed the plots to an AI for analysis
Stage 5. Piecewise analysis based on timeperiods: daily / weekly / monthly / yearly

